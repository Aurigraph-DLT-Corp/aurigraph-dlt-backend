# Advanced Horizontal Pod Autoscaler for 2M+ TPS Production Deployment\n# Includes custom metrics, predictive scaling, and emergency response\n\n---\n# Priority Class for Production Workloads\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: production-high\nvalue: 1000000\nglobalDefault: false\ndescription: \"High priority class for production Aurigraph workloads\"\n\n---\n# Service Account for Production Deployment\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: aurigraph-v11-production\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: security\n\n---\n# RBAC ClusterRole for Production Operations\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: aurigraph-v11-production-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\", \"endpoints\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"metrics.k8s.io\"]\n  resources: [\"pods\", \"nodes\"]\n  verbs: [\"get\", \"list\"]\n\n---\n# ClusterRoleBinding for Service Account\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: aurigraph-v11-production-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: aurigraph-v11-production-role\nsubjects:\n- kind: ServiceAccount\n  name: aurigraph-v11-production\n  namespace: aurigraph-production\n\n---\n# Advanced HPA with Multiple Metrics and Predictive Scaling\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: aurigraph-v11-production-hpa\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: autoscaling\n    environment: production\n  annotations:\n    autoscaling.alpha.kubernetes.io/metrics: |\n      [\n        {\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"target\":{\"type\":\"Utilization\",\"averageUtilization\":60}}},\n        {\"type\":\"Resource\",\"resource\":{\"name\":\"memory\",\"target\":{\"type\":\"Utilization\",\"averageUtilization\":70}}},\n        {\"type\":\"Pods\",\"pods\":{\"metric\":{\"name\":\"aurigraph_transactions_per_second\"},\"target\":{\"type\":\"AverageValue\",\"averageValue\":\"1800000\"}}},\n        {\"type\":\"Pods\",\"pods\":{\"metric\":{\"name\":\"aurigraph_consensus_latency_p95\"},\"target\":{\"type\":\"AverageValue\",\"averageValue\":\"50\"}}}\n      ]\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: aurigraph-v11-production\n  minReplicas: 5   # Production minimum for high availability\n  maxReplicas: 50  # Maximum for ultra-high load\n  metrics:\n  # CPU-based scaling - Conservative for stability\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60  # Lower threshold for production stability\n        \n  # Memory-based scaling\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\n        \n  # TPS-based scaling - Primary metric for Aurigraph\n  - type: Pods\n    pods:\n      metric:\n        name: aurigraph_transactions_per_second\n      target:\n        type: AverageValue\n        averageValue: \"1800000\"  # Scale when TPS > 1.8M per pod\n        \n  # Consensus latency-based scaling\n  - type: Pods\n    pods:\n      metric:\n        name: aurigraph_consensus_latency_p95\n      target:\n        type: AverageValue\n        averageValue: \"50\"  # Scale when p95 latency > 50ms\n        \n  # Network throughput-based scaling\n  - type: Pods\n    pods:\n      metric:\n        name: aurigraph_network_throughput_mbps\n      target:\n        type: AverageValue\n        averageValue: \"5000\"  # Scale when network > 5Gbps per pod\n        \n  # Queue depth-based scaling\n  - type: Pods\n    pods:\n      metric:\n        name: aurigraph_transaction_queue_depth\n      target:\n        type: AverageValue\n        averageValue: \"10000\"  # Scale when queue > 10k transactions\n        \n  # AI optimization load-based scaling\n  - type: Pods\n    pods:\n      metric:\n        name: aurigraph_ai_optimization_load\n      target:\n        type: AverageValue\n        averageValue: \"0.8\"  # Scale when AI optimization load > 80%\n        \n  # HTTP request rate scaling\n  - type: Pods\n    pods:\n      metric:\n        name: http_requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"15000\"  # Scale when HTTP RPS > 15k per pod\n        \n  # gRPC request rate scaling\n  - type: Pods\n    pods:\n      metric:\n        name: grpc_requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"75000\"  # Scale when gRPC RPS > 75k per pod\n  \n  # Advanced scaling behavior for production\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 30   # Quick scale-up for high TPS demands\n      policies:\n      - type: Percent\n        value: 150   # Allow aggressive scale-up (150% increase)\n        periodSeconds: 30\n      - type: Pods\n        value: 10    # Maximum 10 pods at once for scale-up\n        periodSeconds: 30\n      selectPolicy: Max\n    scaleDown:\n      stabilizationWindowSeconds: 600  # Conservative scale-down (10 minutes)\n      policies:\n      - type: Percent\n        value: 25    # Conservative scale-down (25% decrease)\n        periodSeconds: 120\n      - type: Pods\n        value: 3     # Maximum 3 pods at once for scale-down\n        periodSeconds: 120\n      selectPolicy: Min\n\n---\n# Vertical Pod Autoscaler for Resource Optimization\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: aurigraph-v11-production-vpa\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: autoscaling\n    environment: production\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: aurigraph-v11-production\n  updatePolicy:\n    updateMode: \"Off\"  # Use recommendations only, don't auto-apply in production\n  resourcePolicy:\n    containerPolicies:\n    - containerName: aurigraph-v11-production\n      minAllowed:\n        memory: \"128Mi\"   # Native binary minimum\n        cpu: \"250m\"       # Higher minimum for production\n      maxAllowed:\n        memory: \"4Gi\"     # Higher maximum for ultra-high performance\n        cpu: \"16000m\"     # Maximum CPU for extreme TPS loads\n      controlledResources: [\"cpu\", \"memory\"]\n      controlledValues: RequestsAndLimits\n      # Resource scaling mode\n      mode: Auto\n\n---\n# Pod Disruption Budget for High Availability\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: aurigraph-v11-production-pdb\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: availability\n    environment: production\nspec:\n  minAvailable: 60%  # Always keep at least 60% of pods running\n  maxUnavailable: 40%  # Never take down more than 40% at once\n  selector:\n    matchLabels:\n      app: aurigraph-v11\n      deployment-type: production-native\n  unhealthyPodEvictionPolicy: AlwaysAllow\n\n---\n# Network Policy for Production Security\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: aurigraph-v11-production-netpol\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: security\n    environment: production\nspec:\n  podSelector:\n    matchLabels:\n      app: aurigraph-v11\n      deployment-type: production-native\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow traffic from same namespace (inter-pod communication)\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: aurigraph-production\n    ports:\n    - protocol: TCP\n      port: 9003  # HTTP\n    - protocol: TCP\n      port: 9004  # gRPC\n  \n  # Allow traffic from monitoring namespace\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 9003  # Metrics endpoint\n  \n  # Allow traffic from ingress controllers\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    - namespaceSelector:\n        matchLabels:\n          name: istio-system\n    ports:\n    - protocol: TCP\n      port: 9003\n    - protocol: TCP\n      port: 9004\n  \n  # Allow traffic from load balancers\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          app: aws-load-balancer-controller\n    ports:\n    - protocol: TCP\n      port: 9003\n    - protocol: TCP\n      port: 9004\n  \n  egress:\n  # Allow DNS resolution\n  - to: []\n    ports:\n    - protocol: UDP\n      port: 53\n    - protocol: TCP\n      port: 53\n  \n  # Allow communication within namespace\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: aurigraph-production\n    ports:\n    - protocol: TCP\n      port: 9003\n    - protocol: TCP\n      port: 9004\n  \n  # Allow HTTPS outbound for external APIs and health checks\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 443\n    - protocol: TCP\n      port: 80\n  \n  # Allow communication to monitoring and logging systems\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    - namespaceSelector:\n        matchLabels:\n          name: logging\n    ports:\n    - protocol: TCP\n      port: 80\n    - protocol: TCP\n      port: 443\n    - protocol: TCP\n      port: 9090  # Prometheus\n    - protocol: TCP\n      port: 3000  # Grafana\n\n---\n# Service Monitor for Prometheus Scraping\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: aurigraph-v11-production-servicemonitor\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: monitoring\n    environment: production\nspec:\n  selector:\n    matchLabels:\n      app: aurigraph-v11\n      environment: production\n  endpoints:\n  # Primary metrics endpoint\n  - port: metrics\n    path: /q/metrics\n    interval: 10s\n    scrapeTimeout: 8s\n    honorLabels: true\n    metricRelabelings:\n    - sourceLabels: [__name__]\n      regex: 'aurigraph_.*'\n      action: keep\n  \n  # Health check endpoint\n  - port: http\n    path: /q/health\n    interval: 30s\n    scrapeTimeout: 10s\n    metricRelabelings:\n    - sourceLabels: [__name__]\n      regex: 'up|health_.*'\n      action: keep\n  \n  # Performance metrics endpoint\n  - port: http\n    path: /api/v11/stats\n    interval: 15s\n    scrapeTimeout: 12s\n    metricRelabelings:\n    - sourceLabels: [__name__]\n      regex: 'aurigraph_(tps|latency|consensus)_.*'\n      action: keep\n  \n  namespaceSelector:\n    matchNames:\n    - aurigraph-production\n  jobLabel: \"app\"\n\n---\n# Prometheus Rule for Advanced Alerting\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: aurigraph-v11-production-alerts\n  namespace: aurigraph-production\n  labels:\n    app: aurigraph-v11\n    component: alerting\n    environment: production\n    prometheus: kube-prometheus\n    role: alert-rules\nspec:\n  groups:\n  - name: aurigraph-v11-production.performance\n    interval: 15s\n    rules:\n    # Critical performance alerts\n    - alert: AurigraphProductionCriticalTPS\n      expr: rate(aurigraph_transactions_total[2m]) < 1500000\n      for: 2m\n      labels:\n        severity: critical\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production TPS critically low\"\n        description: \"Production TPS is {{ $value | humanize }} TPS, below critical 1.5M threshold for {{ $labels.pod }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/low-tps\"\n        dashboard_url: \"https://grafana.aurigraph.io/d/aurigraph-production\"\n    \n    - alert: AurigraphProductionHighLatency\n      expr: histogram_quantile(0.95, rate(aurigraph_consensus_duration_seconds_bucket[5m])) > 0.1\n      for: 3m\n      labels:\n        severity: critical\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production high consensus latency\"\n        description: \"95th percentile consensus latency is {{ $value }}s, exceeding 100ms SLA\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/high-latency\"\n    \n    - alert: AurigraphProductionMemoryPressure\n      expr: container_memory_usage_bytes{container=\"aurigraph-v11-production\"} / container_spec_memory_limit_bytes{container=\"aurigraph-v11-production\"} > 0.85\n      for: 5m\n      labels:\n        severity: warning\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production high memory usage\"\n        description: \"Memory usage is {{ $value | humanizePercentage }} of limit for {{ $labels.pod }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/memory-pressure\"\n    \n    - alert: AurigraphProductionPodCrashLooping\n      expr: rate(kube_pod_container_status_restarts_total{container=\"aurigraph-v11-production\"}[15m]) > 0\n      for: 5m\n      labels:\n        severity: critical\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production pod crash looping\"\n        description: \"Pod {{ $labels.pod }} has restarted {{ $value }} times in 15 minutes\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/pod-crashloop\"\n    \n    - alert: AurigraphProductionConsensusFailure\n      expr: increase(aurigraph_consensus_failures_total[10m]) > 3\n      for: 2m\n      labels:\n        severity: critical\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production consensus failures\"\n        description: \"{{ $value }} consensus failures in 10 minutes for {{ $labels.pod }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/consensus-failure\"\n    \n    - alert: AurigraphProductionAIOptimizationDown\n      expr: aurigraph_ai_optimization_enabled == 0\n      for: 5m\n      labels:\n        severity: warning\n        service: aurigraph-v11-production\n        team: ai-platform\n      annotations:\n        summary: \"Aurigraph Production AI optimization disabled\"\n        description: \"AI optimization is disabled for {{ $labels.pod }}, performance may be degraded\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/ai-optimization-down\"\n    \n    # Scaling alerts\n    - alert: AurigraphProductionScalingRequired\n      expr: predict_linear(rate(aurigraph_transactions_total[5m])[30m:], 300) > 2500000\n      for: 2m\n      labels:\n        severity: warning\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production scaling required\"\n        description: \"Predicted TPS will exceed 2.5M in 5 minutes, current rate: {{ $value | humanize }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/scaling-required\"\n    \n    # Resource exhaustion alerts\n    - alert: AurigraphProductionDiskSpaceLow\n      expr: (node_filesystem_avail_bytes{mountpoint=\"/app/data\"} / node_filesystem_size_bytes{mountpoint=\"/app/data\"}) < 0.15\n      for: 5m\n      labels:\n        severity: warning\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production disk space low\"\n        description: \"Available disk space is {{ $value | humanizePercentage }} for {{ $labels.instance }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/disk-space-low\"\n    \n    - alert: AurigraphProductionNetworkSaturation\n      expr: rate(container_network_transmit_bytes_total[5m]) + rate(container_network_receive_bytes_total[5m]) > 8000000000\n      for: 3m\n      labels:\n        severity: warning\n        service: aurigraph-v11-production\n        team: platform\n      annotations:\n        summary: \"Aurigraph Production network saturation\"\n        description: \"Network throughput is {{ $value | humanize1024 }}/s approaching 10Gbps limit for {{ $labels.pod }}\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/network-saturation\"\n\n  - name: aurigraph-v11-production.sla\n    interval: 30s\n    rules:\n    # SLA monitoring\n    - alert: AurigraphProductionSLAViolation\n      expr: |\n        (\n          sum(rate(aurigraph_transactions_total[5m])) < 2000000 or\n          histogram_quantile(0.99, rate(aurigraph_transaction_duration_seconds_bucket[5m])) > 0.050\n        )\n      for: 10m\n      labels:\n        severity: critical\n        service: aurigraph-v11-production\n        team: platform\n        alert_type: sla_violation\n      annotations:\n        summary: \"Aurigraph Production SLA violation\"\n        description: \"Either TPS < 2M or p99 latency > 50ms for 10+ minutes\"\n        runbook_url: \"https://docs.aurigraph.io/runbooks/sla-violation\"\n        escalation_policy: \"immediate\"\n"