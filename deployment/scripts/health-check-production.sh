#!/bin/bash\n# Aurigraph V11 Production Health Check and Monitoring Script\n# Comprehensive health validation for production deployment\n\nset -euo pipefail\n\n# =============================================================================\n# Configuration\n# =============================================================================\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPROJECT_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nTIMESTAMP=\"$(date '+%Y%m%d-%H%M%S')\"\nHEALTH_LOG=\"${PROJECT_ROOT}/logs/health-check-${TIMESTAMP}.log\"\n\n# Create logs directory\nmkdir -p \"${PROJECT_ROOT}/logs\"\n\n# Default configuration\nNAMESPACE=\"aurigraph-production\"\nDEPLOYMENT_NAME=\"aurigraph-v11-production\"\nSERVICE_NAME=\"aurigraph-v11-production-service\"\nKUBE_CONTEXT=\"${KUBE_CONTEXT:-production}\"\n\n# Health check thresholds\nMIN_TPS=\"1500000\"  # Minimum acceptable TPS\nMAX_LATENCY_MS=\"100\"  # Maximum acceptable latency in ms\nMIN_READY_REPLICAS=\"3\"  # Minimum ready replicas\nMAX_ERROR_RATE=\"0.01\"  # Maximum error rate (1%)\nMAX_CPU_USAGE=\"80\"  # Maximum CPU usage percentage\nMAX_MEMORY_USAGE=\"85\"  # Maximum memory usage percentage\n\n# Timeout values\nHEALTH_CHECK_TIMEOUT=\"60\"\nPERFORMANCE_TEST_TIMEOUT=\"30\"\nMETRICS_COLLECTION_TIMEOUT=\"15\"\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nMAGENTA='\\033[0;35m'\nCYAN='\\033[0;36m'\nNC='\\033[0m'\n\n# Health check results\nHEALTH_CHECKS_PASSED=0\nHEALTH_CHECKS_FAILED=0\nHEALTH_CHECKS_WARNING=0\n\n# =============================================================================\n# Logging Functions\n# =============================================================================\n\nlog_info() {\n    echo -e \"${BLUE}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*\" | tee -a \"$HEALTH_LOG\"\n}\n\nlog_success() {\n    echo -e \"${GREEN}[PASS]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*\" | tee -a \"$HEALTH_LOG\"\n    ((HEALTH_CHECKS_PASSED++))\n}\n\nlog_warn() {\n    echo -e \"${YELLOW}[WARN]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*\" | tee -a \"$HEALTH_LOG\"\n    ((HEALTH_CHECKS_WARNING++))\n}\n\nlog_fail() {\n    echo -e \"${RED}[FAIL]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*\" | tee -a \"$HEALTH_LOG\"\n    ((HEALTH_CHECKS_FAILED++))\n}\n\nlog_metric() {\n    echo -e \"${CYAN}[METRIC]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*\" | tee -a \"$HEALTH_LOG\"\n}\n\n# =============================================================================\n# Utility Functions\n# =============================================================================\n\ncheck_prerequisites() {\n    log_info \"Checking health check prerequisites...\"\n    \n    # Check required commands\n    for cmd in kubectl jq curl bc; do\n        if ! command -v \"$cmd\" &> /dev/null; then\n            log_fail \"Required command '$cmd' not found\"\n            exit 1\n        fi\n    done\n    \n    # Check Kubernetes context\n    if ! kubectl config get-contexts \"$KUBE_CONTEXT\" &> /dev/null; then\n        log_fail \"Kubernetes context '$KUBE_CONTEXT' not found\"\n        exit 1\n    fi\n    \n    kubectl config use-context \"$KUBE_CONTEXT\" > /dev/null\n    \n    # Verify cluster connectivity\n    if ! kubectl cluster-info &> /dev/null; then\n        log_fail \"Cannot connect to Kubernetes cluster\"\n        exit 1\n    fi\n    \n    log_success \"Prerequisites check completed\"\n}\n\nget_pods() {\n    kubectl get pods -l app=aurigraph-v11 -n \"$NAMESPACE\" -o json 2>/dev/null\n}\n\nget_service_endpoint() {\n    local service_ip\n    service_ip=$(kubectl get service \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo \"\")\n    \n    if [[ -z \"$service_ip\" ]]; then\n        # Try to get external name or hostname\n        service_ip=$(kubectl get service \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo \"\")\n    fi\n    \n    if [[ -z \"$service_ip\" ]]; then\n        # Fall back to cluster IP for internal testing\n        service_ip=$(kubectl get service \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o jsonpath='{.spec.clusterIP}' 2>/dev/null || echo \"\")\n    fi\n    \n    echo \"$service_ip\"\n}\n\nget_random_pod() {\n    local pods_json\n    pods_json=$(get_pods)\n    \n    if [[ \"$(echo \"$pods_json\" | jq '.items | length')\" -eq 0 ]]; then\n        echo \"\"\n        return 1\n    fi\n    \n    echo \"$pods_json\" | jq -r '.items[0].metadata.name'\n}\n\nexec_in_pod() {\n    local pod_name=\"$1\"\n    local command=\"$2\"\n    local timeout=\"${3:-$HEALTH_CHECK_TIMEOUT}\"\n    \n    timeout \"$timeout\" kubectl exec \"$pod_name\" -n \"$NAMESPACE\" -c aurigraph-v11-production -- sh -c \"$command\" 2>/dev/null\n}\n\n# =============================================================================\n# Health Check Functions\n# =============================================================================\n\ncheck_deployment_status() {\n    log_info \"Checking deployment status...\"\n    \n    local deployment_json\n    if ! deployment_json=$(kubectl get deployment \"$DEPLOYMENT_NAME\" -n \"$NAMESPACE\" -o json 2>/dev/null); then\n        log_fail \"Deployment '$DEPLOYMENT_NAME' not found\"\n        return 1\n    fi\n    \n    local desired_replicas\n    local ready_replicas\n    local available_replicas\n    local updated_replicas\n    \n    desired_replicas=$(echo \"$deployment_json\" | jq -r '.spec.replicas')\n    ready_replicas=$(echo \"$deployment_json\" | jq -r '.status.readyReplicas // 0')\n    available_replicas=$(echo \"$deployment_json\" | jq -r '.status.availableReplicas // 0')\n    updated_replicas=$(echo \"$deployment_json\" | jq -r '.status.updatedReplicas // 0')\n    \n    log_metric \"Desired replicas: $desired_replicas\"\n    log_metric \"Ready replicas: $ready_replicas\"\n    log_metric \"Available replicas: $available_replicas\"\n    log_metric \"Updated replicas: $updated_replicas\"\n    \n    # Check if deployment is ready\n    if [[ \"$ready_replicas\" -eq \"$desired_replicas\" ]] && \\\n       [[ \"$available_replicas\" -eq \"$desired_replicas\" ]] && \\\n       [[ \"$updated_replicas\" -eq \"$desired_replicas\" ]]; then\n        log_success \"Deployment is fully ready and updated\"\n    elif [[ \"$ready_replicas\" -ge \"$MIN_READY_REPLICAS\" ]]; then\n        log_warn \"Deployment partially ready ($ready_replicas/$desired_replicas replicas)\"\n    else\n        log_fail \"Deployment not ready ($ready_replicas/$desired_replicas replicas)\"\n    fi\n    \n    # Check deployment conditions\n    local conditions\n    conditions=$(echo \"$deployment_json\" | jq -r '.status.conditions[]? | select(.type == \"Available\" or .type == \"Progressing\") | \"\\(.type): \\(.status) - \\(.message)\"')\n    \n    if [[ -n \"$conditions\" ]]; then\n        log_info \"Deployment conditions:\"\n        echo \"$conditions\" | while IFS= read -r condition; do\n            log_metric \"  $condition\"\n        done\n    fi\n}\n\ncheck_pod_status() {\n    log_info \"Checking pod status...\"\n    \n    local pods_json\n    pods_json=$(get_pods)\n    \n    local pod_count\n    pod_count=$(echo \"$pods_json\" | jq '.items | length')\n    \n    if [[ \"$pod_count\" -eq 0 ]]; then\n        log_fail \"No pods found\"\n        return 1\n    fi\n    \n    log_metric \"Total pods: $pod_count\"\n    \n    local ready_pods=0\n    local running_pods=0\n    local pending_pods=0\n    local failed_pods=0\n    local restart_count=0\n    \n    echo \"$pods_json\" | jq -r '.items[] | \"\\(.metadata.name) \\(.status.phase) \\(.status.conditions[]? | select(.type == \"Ready\") | .status) \\(.status.containerStatuses[]?.restartCount // 0)\"' | while read -r pod_name phase ready restarts; do\n        log_metric \"Pod: $pod_name - Phase: $phase - Ready: $ready - Restarts: $restarts\"\n        \n        case \"$phase\" in\n            \"Running\")\n                ((running_pods++))\n                if [[ \"$ready\" == \"True\" ]]; then\n                    ((ready_pods++))\n                fi\n                ;;\n            \"Pending\")\n                ((pending_pods++))\n                ;;\n            \"Failed\")\n                ((failed_pods++))\n                ;;\n        esac\n        \n        restart_count=$((restart_count + restarts))\n    done\n    \n    log_metric \"Ready pods: $ready_pods\"\n    log_metric \"Running pods: $running_pods\"\n    log_metric \"Pending pods: $pending_pods\"\n    log_metric \"Failed pods: $failed_pods\"\n    log_metric \"Total restarts: $restart_count\"\n    \n    if [[ \"$ready_pods\" -ge \"$MIN_READY_REPLICAS\" ]]; then\n        log_success \"Sufficient pods ready ($ready_pods >= $MIN_READY_REPLICAS)\"\n    else\n        log_fail \"Insufficient pods ready ($ready_pods < $MIN_READY_REPLICAS)\"\n    fi\n    \n    if [[ \"$failed_pods\" -gt 0 ]]; then\n        log_fail \"$failed_pods pods in failed state\"\n    fi\n    \n    if [[ \"$restart_count\" -gt 10 ]]; then\n        log_warn \"High restart count: $restart_count (investigate pod stability)\"\n    elif [[ \"$restart_count\" -gt 0 ]]; then\n        log_info \"Some pod restarts detected: $restart_count\"\n    else\n        log_success \"No pod restarts detected\"\n    fi\n}\n\ncheck_service_status() {\n    log_info \"Checking service status...\"\n    \n    local service_json\n    if ! service_json=$(kubectl get service \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o json 2>/dev/null); then\n        log_fail \"Service '$SERVICE_NAME' not found\"\n        return 1\n    fi\n    \n    local service_type\n    local cluster_ip\n    local external_ip\n    local ports\n    \n    service_type=$(echo \"$service_json\" | jq -r '.spec.type')\n    cluster_ip=$(echo \"$service_json\" | jq -r '.spec.clusterIP')\n    external_ip=$(echo \"$service_json\" | jq -r '.status.loadBalancer.ingress[0].ip // .status.loadBalancer.ingress[0].hostname // \"pending\"')\n    ports=$(echo \"$service_json\" | jq -r '.spec.ports[] | \"\\(.name):\\(.port)->\\(.targetPort)\"' | tr '\\n' ' ')\n    \n    log_metric \"Service type: $service_type\"\n    log_metric \"Cluster IP: $cluster_ip\"\n    log_metric \"External IP: $external_ip\"\n    log_metric \"Ports: $ports\"\n    \n    if [[ \"$service_type\" == \"LoadBalancer\" ]]; then\n        if [[ \"$external_ip\" == \"pending\" ]] || [[ \"$external_ip\" == \"null\" ]]; then\n            log_warn \"LoadBalancer external IP is pending\"\n        else\n            log_success \"LoadBalancer external IP assigned: $external_ip\"\n        fi\n    fi\n    \n    # Check service endpoints\n    local endpoints_json\n    if endpoints_json=$(kubectl get endpoints \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o json 2>/dev/null); then\n        local ready_endpoints\n        local not_ready_endpoints\n        \n        ready_endpoints=$(echo \"$endpoints_json\" | jq '.subsets[]?.addresses // [] | length' | paste -sd+ | bc)\n        not_ready_endpoints=$(echo \"$endpoints_json\" | jq '.subsets[]?.notReadyAddresses // [] | length' | paste -sd+ | bc)\n        \n        log_metric \"Ready endpoints: $ready_endpoints\"\n        log_metric \"Not ready endpoints: $not_ready_endpoints\"\n        \n        if [[ \"$ready_endpoints\" -gt 0 ]]; then\n            log_success \"Service has ready endpoints\"\n        else\n            log_fail \"Service has no ready endpoints\"\n        fi\n    else\n        log_warn \"Cannot retrieve service endpoints\"\n    fi\n}\n\ncheck_application_health() {\n    log_info \"Checking application health endpoints...\"\n    \n    local pod_name\n    if ! pod_name=$(get_random_pod); then\n        log_fail \"No pods available for health check\"\n        return 1\n    fi\n    \n    log_info \"Testing health endpoints on pod: $pod_name\"\n    \n    # Check liveness endpoint\n    if exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/q/health/live\" 10; then\n        log_success \"Liveness endpoint responding\"\n    else\n        log_fail \"Liveness endpoint not responding\"\n    fi\n    \n    # Check readiness endpoint\n    if exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/q/health/ready\" 10; then\n        log_success \"Readiness endpoint responding\"\n    else\n        log_fail \"Readiness endpoint not responding\"\n    fi\n    \n    # Check started endpoint\n    if exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/q/health/started\" 10; then\n        log_success \"Started endpoint responding\"\n    else\n        log_fail \"Started endpoint not responding\"\n    fi\n    \n    # Check general health endpoint\n    local health_response\n    if health_response=$(exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/q/health\" 10); then\n        local health_status\n        health_status=$(echo \"$health_response\" | jq -r '.status // \"unknown\"' 2>/dev/null || echo \"unknown\")\n        \n        if [[ \"$health_status\" == \"UP\" ]]; then\n            log_success \"General health status: UP\"\n        else\n            log_fail \"General health status: $health_status\"\n        fi\n    else\n        log_fail \"General health endpoint not responding\"\n    fi\n}\n\ncheck_performance_metrics() {\n    log_info \"Checking performance metrics...\"\n    \n    local pod_name\n    if ! pod_name=$(get_random_pod); then\n        log_fail \"No pods available for performance check\"\n        return 1\n    fi\n    \n    # Check metrics endpoint\n    local metrics_response\n    if metrics_response=$(exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/q/metrics\" \"$METRICS_COLLECTION_TIMEOUT\"); then\n        log_success \"Metrics endpoint responding\"\n        \n        # Extract key performance metrics\n        local http_requests_total\n        local http_request_duration\n        local jvm_memory_used\n        local system_cpu_usage\n        \n        http_requests_total=$(echo \"$metrics_response\" | grep '^http_server_requests_seconds_count' | head -1 | awk '{print $2}' || echo \"0\")\n        http_request_duration=$(echo \"$metrics_response\" | grep '^http_server_requests_seconds_sum' | head -1 | awk '{print $2}' || echo \"0\")\n        jvm_memory_used=$(echo \"$metrics_response\" | grep '^jvm_memory_used_bytes.*heap' | head -1 | awk '{print $2}' || echo \"0\")\n        system_cpu_usage=$(echo \"$metrics_response\" | grep '^system_cpu_usage' | head -1 | awk '{print $2}' || echo \"0\")\n        \n        log_metric \"HTTP requests total: $http_requests_total\"\n        log_metric \"HTTP request duration sum: $http_request_duration\"\n        log_metric \"JVM memory used: $jvm_memory_used bytes\"\n        log_metric \"System CPU usage: $system_cpu_usage\"\n        \n        # Calculate average response time if possible\n        if [[ \"$http_requests_total\" != \"0\" ]] && [[ \"$http_request_duration\" != \"0\" ]]; then\n            local avg_response_time\n            avg_response_time=$(echo \"scale=3; $http_request_duration / $http_requests_total * 1000\" | bc -l 2>/dev/null || echo \"0\")\n            log_metric \"Average response time: ${avg_response_time}ms\"\n            \n            if (( $(echo \"$avg_response_time < $MAX_LATENCY_MS\" | bc -l) )); then\n                log_success \"Average response time acceptable (${avg_response_time}ms < ${MAX_LATENCY_MS}ms)\"\n            else\n                log_warn \"Average response time high (${avg_response_time}ms >= ${MAX_LATENCY_MS}ms)\"\n            fi\n        fi\n    else\n        log_fail \"Metrics endpoint not responding\"\n    fi\n    \n    # Check performance endpoint\n    if exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/api/v11/performance\" \"$PERFORMANCE_TEST_TIMEOUT\"; then\n        log_success \"Performance endpoint responding\"\n    else\n        log_fail \"Performance endpoint not responding\"\n    fi\n    \n    # Check stats endpoint\n    local stats_response\n    if stats_response=$(exec_in_pod \"$pod_name\" \"curl -sf http://localhost:9003/api/v11/stats\" 15); then\n        log_success \"Stats endpoint responding\"\n        \n        # Try to extract TPS if available\n        local current_tps\n        current_tps=$(echo \"$stats_response\" | jq -r '.currentTPS // .tps // 0' 2>/dev/null || echo \"0\")\n        \n        if [[ \"$current_tps\" != \"0\" ]]; then\n            log_metric \"Current TPS: $current_tps\"\n            \n            if (( $(echo \"$current_tps >= $MIN_TPS\" | bc -l) )); then\n                log_success \"TPS above minimum threshold (${current_tps} >= ${MIN_TPS})\"\n            else\n                log_warn \"TPS below minimum threshold (${current_tps} < ${MIN_TPS})\"\n            fi\n        else\n            log_info \"TPS metrics not available or system not under load\"\n        fi\n    else\n        log_warn \"Stats endpoint not responding - may be expected during low traffic\"\n    fi\n}\n\ncheck_resource_usage() {\n    log_info \"Checking resource usage...\"\n    \n    local pods_json\n    pods_json=$(get_pods)\n    \n    # Check CPU and memory usage for each pod\n    echo \"$pods_json\" | jq -r '.items[].metadata.name' | while read -r pod_name; do\n        log_info \"Checking resources for pod: $pod_name\"\n        \n        # Get resource usage (requires metrics-server)\n        local pod_metrics\n        if pod_metrics=$(kubectl top pod \"$pod_name\" -n \"$NAMESPACE\" --no-headers 2>/dev/null); then\n            local cpu_usage\n            local memory_usage\n            \n            cpu_usage=$(echo \"$pod_metrics\" | awk '{print $2}' | sed 's/m//')\n            memory_usage=$(echo \"$pod_metrics\" | awk '{print $3}' | sed 's/Mi//')\n            \n            log_metric \"Pod $pod_name - CPU: ${cpu_usage}m, Memory: ${memory_usage}Mi\"\n            \n            # Convert to percentages (approximate based on limits)\n            local cpu_limit_m=8000  # 8 cores = 8000m\n            local memory_limit_mb=2048  # 2GB = 2048MB\n            \n            local cpu_percentage\n            local memory_percentage\n            \n            cpu_percentage=$(echo \"scale=1; $cpu_usage * 100 / $cpu_limit_m\" | bc -l 2>/dev/null || echo \"0\")\n            memory_percentage=$(echo \"scale=1; $memory_usage * 100 / $memory_limit_mb\" | bc -l 2>/dev/null || echo \"0\")\n            \n            log_metric \"Pod $pod_name - CPU: ${cpu_percentage}%, Memory: ${memory_percentage}%\"\n            \n            # Check against thresholds\n            if (( $(echo \"$cpu_percentage < $MAX_CPU_USAGE\" | bc -l) )); then\n                log_success \"Pod $pod_name CPU usage acceptable (${cpu_percentage}% < ${MAX_CPU_USAGE}%)\"\n            else\n                log_warn \"Pod $pod_name CPU usage high (${cpu_percentage}% >= ${MAX_CPU_USAGE}%)\"\n            fi\n            \n            if (( $(echo \"$memory_percentage < $MAX_MEMORY_USAGE\" | bc -l) )); then\n                log_success \"Pod $pod_name memory usage acceptable (${memory_percentage}% < ${MAX_MEMORY_USAGE}%)\"\n            else\n                log_warn \"Pod $pod_name memory usage high (${memory_percentage}% >= ${MAX_MEMORY_USAGE}%)\"\n            fi\n        else\n            log_warn \"Cannot retrieve resource metrics for pod $pod_name (metrics-server may not be available)\"\n        fi\n    done\n}\n\ncheck_hpa_status() {\n    log_info \"Checking HPA (Horizontal Pod Autoscaler) status...\"\n    \n    local hpa_name=\"aurigraph-v11-production-hpa\"\n    local hpa_json\n    \n    if ! hpa_json=$(kubectl get hpa \"$hpa_name\" -n \"$NAMESPACE\" -o json 2>/dev/null); then\n        log_warn \"HPA '$hpa_name' not found - autoscaling not configured\"\n        return 0\n    fi\n    \n    local current_replicas\n    local desired_replicas\n    local min_replicas\n    local max_replicas\n    local target_cpu\n    local current_cpu\n    \n    current_replicas=$(echo \"$hpa_json\" | jq -r '.status.currentReplicas // 0')\n    desired_replicas=$(echo \"$hpa_json\" | jq -r '.status.desiredReplicas // 0')\n    min_replicas=$(echo \"$hpa_json\" | jq -r '.spec.minReplicas')\n    max_replicas=$(echo \"$hpa_json\" | jq -r '.spec.maxReplicas')\n    target_cpu=$(echo \"$hpa_json\" | jq -r '.spec.metrics[] | select(.type == \"Resource\" and .resource.name == \"cpu\") | .resource.target.averageUtilization // \"N/A\"')\n    current_cpu=$(echo \"$hpa_json\" | jq -r '.status.currentMetrics[] | select(.type == \"Resource\" and .resource.name == \"cpu\") | .resource.current.averageUtilization // \"N/A\"')\n    \n    log_metric \"Current replicas: $current_replicas\"\n    log_metric \"Desired replicas: $desired_replicas\"\n    log_metric \"Min replicas: $min_replicas\"\n    log_metric \"Max replicas: $max_replicas\"\n    log_metric \"Target CPU utilization: $target_cpu%\"\n    log_metric \"Current CPU utilization: $current_cpu%\"\n    \n    if [[ \"$current_replicas\" -eq \"$desired_replicas\" ]]; then\n        log_success \"HPA replicas stable ($current_replicas = $desired_replicas)\"\n    else\n        log_info \"HPA scaling in progress ($current_replicas -> $desired_replicas)\"\n    fi\n    \n    if [[ \"$current_replicas\" -ge \"$min_replicas\" ]] && [[ \"$current_replicas\" -le \"$max_replicas\" ]]; then\n        log_success \"HPA replicas within bounds ($min_replicas <= $current_replicas <= $max_replicas)\"\n    else\n        log_warn \"HPA replicas outside bounds ($current_replicas not in [$min_replicas, $max_replicas])\"\n    fi\n    \n    # Check HPA conditions\n    local conditions\n    conditions=$(echo \"$hpa_json\" | jq -r '.status.conditions[]? | \"\\(.type): \\(.status) - \\(.message)\"')\n    \n    if [[ -n \"$conditions\" ]]; then\n        log_info \"HPA conditions:\"\n        echo \"$conditions\" | while IFS= read -r condition; do\n            log_metric \"  $condition\"\n        done\n    fi\n}\n\ncheck_network_connectivity() {\n    log_info \"Checking network connectivity...\"\n    \n    local pod_name\n    if ! pod_name=$(get_random_pod); then\n        log_fail \"No pods available for network check\"\n        return 1\n    fi\n    \n    # Check internal connectivity (pod to service)\n    if exec_in_pod \"$pod_name\" \"curl -sf http://${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local:9003/q/health\" 10; then\n        log_success \"Internal service connectivity working\"\n    else\n        log_fail \"Internal service connectivity failed\"\n    fi\n    \n    # Check DNS resolution\n    if exec_in_pod \"$pod_name\" \"nslookup ${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local\" 10; then\n        log_success \"DNS resolution working\"\n    else\n        log_fail \"DNS resolution failed\"\n    fi\n    \n    # Check external connectivity (if configured)\n    local service_endpoint\n    service_endpoint=$(get_service_endpoint)\n    \n    if [[ -n \"$service_endpoint\" ]] && [[ \"$service_endpoint\" != \"<none>\" ]]; then\n        log_info \"Testing external connectivity via $service_endpoint\"\n        \n        if timeout 15 curl -sf \"http://$service_endpoint:9003/q/health\" > /dev/null 2>&1; then\n            log_success \"External connectivity working\"\n        else\n            log_warn \"External connectivity failed - may be expected if load balancer is not ready\"\n        fi\n    else\n        log_info \"No external endpoint available for testing\"\n    fi\n}\n\ngenerate_health_report() {\n    log_info \"Generating health check report...\"\n    \n    local report_file=\"${PROJECT_ROOT}/logs/health-report-${TIMESTAMP}.json\"\n    local summary_file=\"${PROJECT_ROOT}/logs/health-summary-${TIMESTAMP}.txt\"\n    \n    # Collect comprehensive system state\n    local deployment_info\n    local pods_info\n    local service_info\n    local hpa_info\n    \n    deployment_info=$(kubectl get deployment \"$DEPLOYMENT_NAME\" -n \"$NAMESPACE\" -o json 2>/dev/null || echo '{}')\n    pods_info=$(get_pods)\n    service_info=$(kubectl get service \"$SERVICE_NAME\" -n \"$NAMESPACE\" -o json 2>/dev/null || echo '{}')\n    hpa_info=$(kubectl get hpa aurigraph-v11-production-hpa -n \"$NAMESPACE\" -o json 2>/dev/null || echo '{}')\n    \n    # Generate JSON report\n    cat > \"$report_file\" <<EOF\n{\n  \"health_check\": {\n    \"timestamp\": \"$TIMESTAMP\",\n    \"namespace\": \"$NAMESPACE\",\n    \"deployment_name\": \"$DEPLOYMENT_NAME\",\n    \"service_name\": \"$SERVICE_NAME\",\n    \"checks_passed\": $HEALTH_CHECKS_PASSED,\n    \"checks_failed\": $HEALTH_CHECKS_FAILED,\n    \"checks_warning\": $HEALTH_CHECKS_WARNING,\n    \"overall_status\": \"$([ $HEALTH_CHECKS_FAILED -eq 0 ] && echo 'healthy' || echo 'unhealthy')\"\n  },\n  \"kubernetes\": {\n    \"deployment\": $deployment_info,\n    \"pods\": $pods_info,\n    \"service\": $service_info,\n    \"hpa\": $hpa_info\n  },\n  \"thresholds\": {\n    \"min_tps\": $MIN_TPS,\n    \"max_latency_ms\": $MAX_LATENCY_MS,\n    \"min_ready_replicas\": $MIN_READY_REPLICAS,\n    \"max_error_rate\": $MAX_ERROR_RATE,\n    \"max_cpu_usage\": $MAX_CPU_USAGE,\n    \"max_memory_usage\": $MAX_MEMORY_USAGE\n  }\n}\nEOF\n    \n    # Generate summary report\n    cat > \"$summary_file\" <<EOF\nAurigraph V11 Production Health Check Summary\n=============================================\nTimestamp: $TIMESTAMP\nNamespace: $NAMESPACE\n\nHealth Check Results:\n- Passed: $HEALTH_CHECKS_PASSED\n- Failed: $HEALTH_CHECKS_FAILED\n- Warnings: $HEALTH_CHECKS_WARNING\n- Overall Status: $([ $HEALTH_CHECKS_FAILED -eq 0 ] && echo 'HEALTHY' || echo 'UNHEALTHY')\n\nThresholds:\n- Minimum TPS: $MIN_TPS\n- Maximum Latency: ${MAX_LATENCY_MS}ms\n- Minimum Ready Replicas: $MIN_READY_REPLICAS\n- Maximum CPU Usage: ${MAX_CPU_USAGE}%\n- Maximum Memory Usage: ${MAX_MEMORY_USAGE}%\n\nDetailed log: $HEALTH_LOG\nFull report: $report_file\nEOF\n    \n    log_success \"Health check report generated: $report_file\"\n    log_success \"Health check summary: $summary_file\"\n}\n\nsend_health_notification() {\n    local overall_status\n    overall_status=$([ $HEALTH_CHECKS_FAILED -eq 0 ] && echo 'healthy' || echo 'unhealthy')\n    \n    log_info \"Sending health check notification...\"\n    \n    # Slack notification\n    if [[ -n \"${SLACK_WEBHOOK_URL:-}\" ]]; then\n        local color=\"good\"\n        [[ \"$overall_status\" != \"healthy\" ]] && color=\"danger\"\n        \n        curl -X POST -H 'Content-type: application/json' \\\n            --data '{\n                \"attachments\": [{\n                    \"color\": \"'$color'\",\n                    \"title\": \"Aurigraph V11 Production Health Check\",\n                    \"text\": \"Health check completed - Status: '$overall_status'\",\n                    \"fields\": [\n                        {\"title\": \"Passed\", \"value\": \"'$HEALTH_CHECKS_PASSED'\", \"short\": true},\n                        {\"title\": \"Failed\", \"value\": \"'$HEALTH_CHECKS_FAILED'\", \"short\": true},\n                        {\"title\": \"Warnings\", \"value\": \"'$HEALTH_CHECKS_WARNING'\", \"short\": true},\n                        {\"title\": \"Namespace\", \"value\": \"'$NAMESPACE'\", \"short\": true}\n                    ],\n                    \"footer\": \"Aurigraph V11 Health Check\",\n                    \"ts\": '$(date +%s)'\n                }]\n            }' \\\n            \"$SLACK_WEBHOOK_URL\" 2>/dev/null || true\n    fi\n}\n\n# =============================================================================\n# Main Health Check Process\n# =============================================================================\n\nmain() {\n    local continuous_mode=false\n    local interval_seconds=300  # 5 minutes\n    local notification_enabled=true\n    \n    # Parse command line arguments\n    while [[ $# -gt 0 ]]; do\n        case $1 in\n            --namespace)\n                NAMESPACE=\"$2\"\n                shift 2\n                ;;\n            --context)\n                KUBE_CONTEXT=\"$2\"\n                shift 2\n                ;;\n            --continuous)\n                continuous_mode=true\n                shift\n                ;;\n            --interval)\n                interval_seconds=\"$2\"\n                shift 2\n                ;;\n            --min-tps)\n                MIN_TPS=\"$2\"\n                shift 2\n                ;;\n            --max-latency)\n                MAX_LATENCY_MS=\"$2\"\n                shift 2\n                ;;\n            --no-notifications)\n                notification_enabled=false\n                shift\n                ;;\n            --help)\n                echo \"Usage: $0 [options]\"\n                echo \"Options:\"\n                echo \"  --namespace NS        Kubernetes namespace (default: aurigraph-production)\"\n                echo \"  --context CTX         Kubernetes context (default: production)\"\n                echo \"  --continuous          Run health checks continuously\"\n                echo \"  --interval SEC        Interval for continuous checks in seconds (default: 300)\"\n                echo \"  --min-tps TPS         Minimum acceptable TPS (default: $MIN_TPS)\"\n                echo \"  --max-latency MS      Maximum acceptable latency in ms (default: $MAX_LATENCY_MS)\"\n                echo \"  --no-notifications    Disable notifications\"\n                echo \"  --help                Show this help message\"\n                echo \"\"\n                echo \"Examples:\"\n                echo \"  $0                           # Single health check\"\n                echo \"  $0 --continuous              # Continuous monitoring\"\n                echo \"  $0 --min-tps 2000000         # Custom TPS threshold\"\n                exit 0\n                ;;\n            *)\n                log_fail \"Unknown option: $1\"\n                exit 1\n                ;;\n        esac\n    done\n    \n    log_info \"Starting Aurigraph V11 Production Health Check\"\n    log_info \"Timestamp: $TIMESTAMP\"\n    log_info \"Namespace: $NAMESPACE\"\n    log_info \"Context: $KUBE_CONTEXT\"\n    log_info \"Continuous mode: $continuous_mode\"\n    \n    if [[ \"$continuous_mode\" == true ]]; then\n        log_info \"Running continuous health checks (interval: ${interval_seconds}s)\"\n        \n        while true; do\n            run_health_checks\n            \n            if [[ \"$notification_enabled\" == true ]]; then\n                send_health_notification\n            fi\n            \n            log_info \"Next health check in ${interval_seconds}s...\"\n            sleep \"$interval_seconds\"\n            \n            # Reset counters for next iteration\n            HEALTH_CHECKS_PASSED=0\n            HEALTH_CHECKS_FAILED=0\n            HEALTH_CHECKS_WARNING=0\n        done\n    else\n        run_health_checks\n        \n        if [[ \"$notification_enabled\" == true ]]; then\n            send_health_notification\n        fi\n    fi\n}\n\nrun_health_checks() {\n    local health_start=$(date +%s)\n    \n    # Prerequisites\n    check_prerequisites\n    \n    # Core Kubernetes checks\n    check_deployment_status\n    check_pod_status\n    check_service_status\n    \n    # Application-specific checks\n    check_application_health\n    check_performance_metrics\n    \n    # Resource and scaling checks\n    check_resource_usage\n    check_hpa_status\n    \n    # Network connectivity checks\n    check_network_connectivity\n    \n    # Generate reports\n    generate_health_report\n    \n    local health_end=$(date +%s)\n    local health_duration=$((health_end - health_start))\n    \n    # Summary\n    echo -e \"\\n${MAGENTA}=== Health Check Summary ===${NC}\"\n    echo -e \"${GREEN}Passed: $HEALTH_CHECKS_PASSED${NC}\"\n    echo -e \"${YELLOW}Warnings: $HEALTH_CHECKS_WARNING${NC}\"\n    echo -e \"${RED}Failed: $HEALTH_CHECKS_FAILED${NC}\"\n    echo -e \"Duration: ${health_duration}s\"\n    echo -e \"Overall Status: $([ $HEALTH_CHECKS_FAILED -eq 0 ] && echo -e \"${GREEN}HEALTHY${NC}\" || echo -e \"${RED}UNHEALTHY${NC}\")\"\n    echo -e \"Log file: $HEALTH_LOG\"\n    \n    # Exit with appropriate code\n    if [[ $HEALTH_CHECKS_FAILED -gt 0 ]]; then\n        return 1\n    else\n        return 0\n    fi\n}\n\n# Script entry point\nif [[ \"${BASH_SOURCE[0]}\" == \"${0}\" ]]; then\n    main \"$@\"\nfi\n"